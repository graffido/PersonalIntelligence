"""
å¢å¼ºç‰ˆMLæœåŠ¡ä¸»å…¥å£
æ•´åˆNERã€Embeddingå’ŒLLMæœåŠ¡
"""
import os
import yaml
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from contextlib import asynccontextmanager
from datetime import datetime

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import uvicorn

from enhanced_ner import (
    EnhancedNER, Entity, EntityType, RelationType, 
    NERResult, create_ner_service, quick_extract
)
from embedding_service import (
    EmbeddingService, EmbeddingResult, 
    create_embedding_service, quick_encode
)
from llm_service import (
    LLMService, Message, ModelType, TaskComplexity,
    create_llm_service, quick_chat
)


# åŠ è½½é…ç½®
def load_config(config_path: str = "config.yaml") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = Path(config_path)
    
    if config_file.exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    # é»˜è®¤é…ç½®
    return {
        "server": {
            "host": "0.0.0.0",
            "port": 8000,
            "reload": False
        },
        "ner": {
            "use_bert_chinese": False,
            "use_transformer": True,
            "use_ontology": True,
            "ontology_path": "ontology_db"
        },
        "embedding": {
            "default_model": "all-MiniLM-L6-v2",
            "device": "auto",
            "use_cache": True,
            "normalize": True
        },
        "llm": {
            "default_model": "gpt-4o-mini",
            "default_provider": "openai",
            "use_cache": True,
            "cache_size": 1000,
            "routing": {
                "simple": "ollama",
                "moderate": "ollama",
                "complex": "openai"
            }
        }
    }


# å…¨å±€æœåŠ¡å®ä¾‹
ner_service: Optional[EnhancedNER] = None
embedding_service: Optional[EmbeddingService] = None
llm_service: Optional[LLMService] = None
service_config: Dict = {}


# Pydanticæ¨¡å‹å®šä¹‰
class NERRequest(BaseModel):
    text: str = Field(..., description="è¾“å…¥æ–‡æœ¬")
    extract_relations: bool = Field(True, description="æ˜¯å¦æŠ½å–å…³ç³»")
    use_ontology: Optional[bool] = Field(None, description="æ˜¯å¦ä½¿ç”¨Ontologyå¢å¼º")


class NERBatchRequest(BaseModel):
    texts: List[str] = Field(..., description="æ–‡æœ¬åˆ—è¡¨")
    extract_relations: bool = Field(False, description="æ˜¯å¦æŠ½å–å…³ç³»")


class EntityAddRequest(BaseModel):
    text: str = Field(..., description="å®ä½“æ–‡æœ¬")
    entity_type: str = Field(..., description="å®ä½“ç±»å‹")
    metadata: Optional[Dict] = Field(None, description="å…ƒæ•°æ®")


class SynonymAddRequest(BaseModel):
    synonym: str = Field(..., description="åŒä¹‰è¯")
    canonical: str = Field(..., description="æ ‡å‡†è¯")


class EmbeddingRequest(BaseModel):
    texts: Union[str, List[str]] = Field(..., description="è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨")
    model: Optional[str] = Field(None, description="æ¨¡å‹ID")
    batch_size: int = Field(32, description="æ‰¹å¤„ç†å¤§å°")


class SimilarityRequest(BaseModel):
    text1: str = Field(..., description="æ–‡æœ¬1")
    text2: str = Field(..., description="æ–‡æœ¬2")
    model: Optional[str] = Field(None, description="æ¨¡å‹ID")


class SearchRequest(BaseModel):
    query: str = Field(..., description="æŸ¥è¯¢æ–‡æœ¬")
    documents: List[Dict[str, Any]] = Field(..., description="æ–‡æ¡£åˆ—è¡¨")
    top_k: int = Field(5, description="è¿”å›æ•°é‡")
    model: Optional[str] = Field(None, description="æ¨¡å‹ID")


class ClusteringRequest(BaseModel):
    texts: List[str] = Field(..., description="æ–‡æœ¬åˆ—è¡¨")
    n_clusters: int = Field(5, description="èšç±»æ•°é‡")
    model: Optional[str] = Field(None, description="æ¨¡å‹ID")


class ChatRequest(BaseModel):
    messages: Union[str, List[Dict[str, str]]] = Field(..., description="æ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨")
    provider: Optional[str] = Field(None, description="Providerç±»å‹")
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°")
    temperature: float = Field(0.7, description="æ¸©åº¦å‚æ•°")
    max_tokens: int = Field(1024, description="æœ€å¤§tokenæ•°")
    stream: bool = Field(False, description="æ˜¯å¦æµå¼è¾“å‡º")
    use_cache: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨ç¼“å­˜")


class ChatMessage(BaseModel):
    role: str = Field(..., description="è§’è‰²: system/user/assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


# ç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global ner_service, embedding_service, llm_service, service_config
    
    # å¯åŠ¨æ—¶åŠ è½½é…ç½®å’ŒæœåŠ¡
    service_config = load_config()
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨MLæœåŠ¡...")
    print(f"é…ç½®: {json.dumps(service_config, indent=2, default=str)}")
    
    # åˆå§‹åŒ–æœåŠ¡
    print("\nğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æœåŠ¡...")
    
    # NERæœåŠ¡
    try:
        ner_service = create_ner_service(service_config.get("ner", {}))
        ner_service.load_models()
        print("âœ… NERæœåŠ¡å·²åˆå§‹åŒ–")
    except Exception as e:
        print(f"âš ï¸ NERæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # EmbeddingæœåŠ¡
    try:
        embedding_service = create_embedding_service(service_config.get("embedding", {}))
        print("âœ… EmbeddingæœåŠ¡å·²åˆå§‹åŒ–")
    except Exception as e:
        print(f"âš ï¸ EmbeddingæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # LLMæœåŠ¡
    try:
        llm_service = create_llm_service(service_config.get("llm", {}))
        providers = llm_service.get_available_providers()
        print(f"âœ… LLMæœåŠ¡å·²åˆå§‹åŒ–ï¼Œå¯ç”¨providers: {[p['type'] for p in providers]}")
    except Exception as e:
        print(f"âš ï¸ LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    
    print("\nğŸ‰ æ‰€æœ‰æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    print("\nğŸ›‘ æ­£åœ¨å…³é—­æœåŠ¡...")
    if ner_service:
        ner_service.save_ontology()
        print("ğŸ’¾ Ontologyæ•°æ®å·²ä¿å­˜")
    print("ğŸ‘‹ æœåŠ¡å·²å…³é—­")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="å¢å¼ºç‰ˆMLæœåŠ¡",
    description="é›†æˆNERã€Embeddingå’ŒLLMçš„æœºå™¨å­¦ä¹ æœåŠ¡",
    version="2.0.0",
    lifespan=lifespan
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========== NER API ==========

@app.post("/ner/extract", response_model=Dict)
async def ner_extract(request: NERRequest):
    """æŠ½å–å‘½åå®ä½“"""
    if not ner_service:
        raise HTTPException(status_code=503, detail="NERæœåŠ¡ä¸å¯ç”¨")
    
    try:
        result = ner_service.extract(
            text=request.text,
            extract_relations=request.extract_relations,
            use_ontology=request.use_ontology
        )
        return result.to_dict()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/ner/extract_batch", response_model=List[Dict])
async def ner_extract_batch(request: NERBatchRequest):
    """æ‰¹é‡æŠ½å–å‘½åå®ä½“"""
    if not ner_service:
        raise HTTPException(status_code=503, detail="NERæœåŠ¡ä¸å¯ç”¨")
    
    try:
        results = ner_service.extract_batch(
            texts=request.texts,
            extract_relations=request.extract_relations
        )
        return [r.to_dict() for r in results]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/ner/ontology/entity")
async def ner_add_entity(request: EntityAddRequest):
    """æ·»åŠ å®ä½“åˆ°Ontology"""
    if not ner_service:
        raise HTTPException(status_code=503, detail="NERæœåŠ¡ä¸å¯ç”¨")
    
    try:
        ner_service.add_to_ontology(
            text=request.text,
            entity_type=request.entity_type,
            metadata=request.metadata
        )
        return {"status": "success", "message": f"å®ä½“ '{request.text}' å·²æ·»åŠ åˆ°Ontology"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/ner/ontology/synonym")
async def ner_add_synonym(request: SynonymAddRequest):
    """æ·»åŠ åŒä¹‰è¯"""
    if not ner_service:
        raise HTTPException(status_code=503, detail="NERæœåŠ¡ä¸å¯ç”¨")
    
    try:
        ner_service.add_synonym(request.synonym, request.canonical)
        return {"status": "success", "message": f"åŒä¹‰è¯æ˜ å°„å·²æ·»åŠ "}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/ner/ontology/stats")
async def ner_ontology_stats():
    """è·å–Ontologyç»Ÿè®¡ä¿¡æ¯"""
    if not ner_service:
        raise HTTPException(status_code=503, detail="NERæœåŠ¡ä¸å¯ç”¨")
    
    return ner_service.get_ontology_stats()


@app.get("/ner/entity/{text}")
async def ner_get_entity(text: str):
    """è·å–å®ä½“è¯¦ç»†ä¿¡æ¯"""
    if not ner_service:
        raise HTTPException(status_code=503, detail="NERæœåŠ¡ä¸å¯ç”¨")
    
    info = ner_service.get_entity_info(text)
    if info:
        return info
    raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°å®ä½“: {text}")


@app.get("/ner/entity_types")
async def ner_entity_types():
    """è·å–æ”¯æŒçš„å®ä½“ç±»å‹"""
    return {
        "types": [
            {"name": t.value, "description": get_entity_type_desc(t)}
            for t in EntityType
        ]
    }


def get_entity_type_desc(entity_type: EntityType) -> str:
    """è·å–å®ä½“ç±»å‹æè¿°"""
    descriptions = {
        EntityType.PERSON: "äººåã€äººç‰©",
        EntityType.PLACE: "åœ°ç‚¹ã€ä½ç½®",
        EntityType.EVENT: "äº‹ä»¶ã€æ´»åŠ¨",
        EntityType.CONCEPT: "æ¦‚å¿µã€ä¸»é¢˜",
        EntityType.ORGANIZATION: "ç»„ç»‡ã€å…¬å¸ã€æœºæ„",
        EntityType.TIME: "æ—¶é—´ã€æ—¥æœŸ",
        EntityType.MONEY: "é‡‘é¢ã€è´§å¸",
        EntityType.PRODUCT: "äº§å“ã€ç‰©å“",
        EntityType.WORK_OF_ART: "è‰ºæœ¯ä½œå“ã€ä¹¦ç±ã€ç”µå½±",
        EntityType.CUSTOM: "è‡ªå®šä¹‰ç±»å‹"
    }
    return descriptions.get(entity_type, "")


# ========== Embedding API ==========

@app.post("/embedding/encode")
async def embedding_encode(request: EmbeddingRequest):
    """ç¼–ç æ–‡æœ¬ä¸ºembeddingå‘é‡"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    try:
        results = embedding_service.encode(
            texts=request.texts,
            model_id=request.model,
            batch_size=request.batch_size
        )
        
        if isinstance(results, list):
            return {
                "embeddings": [
                    {
                        "text": r.text[:100] + "..." if len(r.text) > 100 else r.text,
                        "embedding": r.embedding.tolist(),
                        "dimension": r.dimension,
                        "model": r.model
                    }
                    for r in results
                ],
                "count": len(results)
            }
        else:
            return {
                "embedding": results.embedding.tolist(),
                "dimension": results.dimension,
                "model": results.model
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embedding/similarity")
async def embedding_similarity(request: SimilarityRequest):
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    try:
        similarity = embedding_service.similarity(
            text1=request.text1,
            text2=request.text2,
            model_id=request.model
        )
        return {
            "text1": request.text1,
            "text2": request.text2,
            "similarity": similarity,
            "model": request.model or embedding_service.default_model
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embedding/search")
async def embedding_search(request: SearchRequest):
    """è¯­ä¹‰æœç´¢"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    try:
        results = embedding_service.semantic_search(
            query=request.query,
            documents=request.documents,
            top_k=request.top_k,
            model_id=request.model
        )
        return {
            "query": request.query,
            "results": results,
            "total": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embedding/cluster")
async def embedding_cluster(request: ClusteringRequest):
    """æ–‡æœ¬èšç±»"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    try:
        clusters = embedding_service.clustering(
            texts=request.texts,
            n_clusters=request.n_clusters,
            model_id=request.model
        )
        return {"clusters": clusters}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/embedding/models")
async def embedding_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    return {"models": embedding_service.list_models()}


@app.get("/embedding/cache/stats")
async def embedding_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    return embedding_service.get_cache_stats() or {"enabled": False}


@app.delete("/embedding/cache")
async def embedding_clear_cache():
    """æ¸…ç©ºç¼“å­˜"""
    if not embedding_service:
        raise HTTPException(status_code=503, detail="EmbeddingæœåŠ¡ä¸å¯ç”¨")
    
    embedding_service.clear_cache()
    return {"status": "success", "message": "ç¼“å­˜å·²æ¸…ç©º"}


# ========== LLM API ==========

@app.post("/llm/chat")
async def llm_chat(request: ChatRequest):
    """LLMèŠå¤©"""
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLMæœåŠ¡ä¸å¯ç”¨")
    
    try:
        # å¤„ç†æ¶ˆæ¯æ ¼å¼
        if isinstance(request.messages, str):
            messages = [Message(role="user", content=request.messages)]
        else:
            messages = [Message(role=m["role"], content=m["content"]) for m in request.messages]
        
        # é€‰æ‹©provider
        provider = None
        if request.provider:
            provider = ModelType(request.provider)
        
        # æµå¼å“åº”
        if request.stream:
            def generate():
                for chunk in llm_service.stream_chat(
                    messages=messages,
                    provider=provider,
                    model=request.model,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens
                ):
                    yield f"data: {json.dumps({'content': chunk})}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate(),
                media_type="text/event-stream"
            )
        
        # æ™®é€šå“åº”
        response = llm_service.chat(
            messages=messages,
            provider=provider,
            model=request.model,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            use_cache=request.use_cache
        )
        
        return response.to_dict()
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/llm/chat/simple")
async def llm_chat_simple(message: str, provider: Optional[str] = None):
    """ç®€åŒ–ç‰ˆèŠå¤©æ¥å£"""
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLMæœåŠ¡ä¸å¯ç”¨")
    
    try:
        provider_type = ModelType(provider) if provider else None
        content = llm_service.simple_chat(message, provider=provider_type)
        return {"content": content}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/llm/providers")
async def llm_providers():
    """è·å–å¯ç”¨providers"""
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLMæœåŠ¡ä¸å¯ç”¨")
    
    return {"providers": llm_service.get_available_providers()}


@app.get("/llm/stats")
async def llm_stats():
    """è·å–LLMä½¿ç”¨ç»Ÿè®¡"""
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLMæœåŠ¡ä¸å¯ç”¨")
    
    return llm_service.get_stats()


@app.delete("/llm/cache")
async def llm_clear_cache():
    """æ¸…ç©ºLLMç¼“å­˜"""
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLMæœåŠ¡ä¸å¯ç”¨")
    
    llm_service.clear_cache()
    return {"status": "success", "message": "ç¼“å­˜å·²æ¸…ç©º"}


# ========== ç³»ç»ŸAPI ==========

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "å¢å¼ºç‰ˆMLæœåŠ¡",
        "version": "2.0.0",
        "docs": "/docs",
        "endpoints": {
            "ner": "/ner/*",
            "embedding": "/embedding/*",
            "llm": "/llm/*"
        }
    }


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "ner": ner_service is not None,
            "embedding": embedding_service is not None,
            "llm": llm_service is not None and len(llm_service.get_available_providers()) > 0
        }
    }


@app.get("/config")
async def get_config():
    """è·å–å½“å‰é…ç½®ï¼ˆè„±æ•ï¼‰"""
    safe_config = json.loads(json.dumps(service_config, default=str))
    
    # ç§»é™¤æ•æ„Ÿä¿¡æ¯
    for section in ["openai", "anthropic"]:
        if section in safe_config.get("llm", {}):
            safe_config["llm"][section]["api_key"] = "***" if safe_config["llm"][section].get("api_key") else None
    
    return safe_config


# ========== å·¥å…·å‡½æ•° ==========

def save_config(config: Dict, path: str = "config.yaml"):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    with open(path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
    default_config = {
        "server": {
            "host": "0.0.0.0",
            "port": 8000,
            "reload": False
        },
        "ner": {
            "use_bert_chinese": False,
            "chinese_bert_model": "bert-base-chinese",
            "use_transformer": True,
            "english_model": "en_core_web_trf",
            "use_ontology": True,
            "ontology_path": "ontology_db"
        },
        "embedding": {
            "default_model": "all-MiniLM-L6-v2",
            "device": "auto",
            "use_cache": True,
            "normalize": True,
            "cache_dir": "embedding_cache"
        },
        "llm": {
            "default_model": "gpt-4o-mini",
            "default_provider": "openai",
            "use_cache": True,
            "cache_size": 1000,
            "cache_ttl": 3600,
            "openai": {
                "enabled": True,
                "model": "gpt-4o-mini",
                "api_key": os.getenv("OPENAI_API_KEY", ""),
                "base_url": "https://api.openai.com/v1",
                "temperature": 0.7,
                "max_tokens": 1024
            },
            "anthropic": {
                "enabled": True,
                "model": "claude-3-haiku-20240307",
                "api_key": os.getenv("ANTHROPIC_API_KEY", ""),
                "temperature": 0.7,
                "max_tokens": 1024
            },
            "ollama": {
                "enabled": True,
                "model": "llama3.2",
                "base_url": "http://localhost:11434",
                "temperature": 0.7,
                "max_tokens": 1024
            },
            "llama_cpp": {
                "enabled": False,
                "model_path": "",
                "n_ctx": 4096,
                "n_gpu_layers": 0
            },
            "routing": {
                "simple": "ollama",
                "moderate": "ollama",
                "complex": "openai"
            }
        }
    }
    
    save_config(default_config)
    return default_config


# ========== ä¸»å…¥å£ ==========

if __name__ == "__main__":
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶...")
        create_default_config()
    
    # åŠ è½½é…ç½®
    config = load_config()
    server_config = config.get("server", {})
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "main:app",
        host=server_config.get("host", "0.0.0.0"),
        port=server_config.get("port", 8000),
        reload=server_config.get("reload", False)
    )
