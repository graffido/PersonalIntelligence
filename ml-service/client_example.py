#!/usr/bin/env python3
"""
MLæœåŠ¡å®¢æˆ·ç«¯ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªAPI
"""
import json
import requests
from typing import Dict, List, Union


class MLServiceClient:
    """MLæœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url.rstrip("/")
    
    # ========== NERæ–¹æ³• ==========
    
    def extract_entities(self, text: str, extract_relations: bool = True) -> Dict:
        """æŠ½å–å‘½åå®ä½“"""
        response = requests.post(
            f"{self.base_url}/ner/extract",
            json={"text": text, "extract_relations": extract_relations}
        )
        response.raise_for_status()
        return response.json()
    
    def add_to_ontology(self, text: str, entity_type: str, metadata: Dict = None) -> Dict:
        """æ·»åŠ å®ä½“åˆ°Ontology"""
        response = requests.post(
            f"{self.base_url}/ner/ontology/entity",
            json={"text": text, "entity_type": entity_type, "metadata": metadata}
        )
        response.raise_for_status()
        return response.json()
    
    def get_ontology_stats(self) -> Dict:
        """è·å–Ontologyç»Ÿè®¡"""
        response = requests.get(f"{self.base_url}/ner/ontology/stats")
        response.raise_for_status()
        return response.json()
    
    # ========== Embeddingæ–¹æ³• ==========
    
    def encode(self, texts: Union[str, List[str]], model: str = None) -> Dict:
        """ç¼–ç æ–‡æœ¬"""
        response = requests.post(
            f"{self.base_url}/embedding/encode",
            json={"texts": texts, "model": model}
        )
        response.raise_for_status()
        return response.json()
    
    def similarity(self, text1: str, text2: str, model: str = None) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        response = requests.post(
            f"{self.base_url}/embedding/similarity",
            json={"text1": text1, "text2": text2, "model": model}
        )
        response.raise_for_status()
        return response.json()["similarity"]
    
    def search(self, query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æœç´¢"""
        response = requests.post(
            f"{self.base_url}/embedding/search",
            json={"query": query, "documents": documents, "top_k": top_k}
        )
        response.raise_for_status()
        return response.json()["results"]
    
    # ========== LLMæ–¹æ³• ==========
    
    def chat(self, message: str, provider: str = None, stream: bool = False) -> str:
        """èŠå¤©"""
        if stream:
            response = requests.post(
                f"{self.base_url}/llm/chat",
                json={"messages": message, "provider": provider, "stream": True},
                stream=True
            )
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data == '[DONE]':
                            break
                        try:
                            chunk = json.loads(data)
                            print(chunk.get('content', ''), end='', flush=True)
                        except:
                            pass
            print()
            return ""
        else:
            response = requests.post(
                f"{self.base_url}/llm/chat/simple",
                params={"message": message, "provider": provider}
            )
            response.raise_for_status()
            return response.json()["content"]
    
    def health(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        response = requests.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()


# ========== ç¤ºä¾‹ç”¨æ³• ==========

def demo_ner(client: MLServiceClient):
    """NERæ¼”ç¤º"""
    print("\n" + "="*50)
    print("ğŸ” NERå®ä½“æŠ½å–æ¼”ç¤º")
    print("="*50)
    
    # ä¸­æ–‡ç¤ºä¾‹
    chinese_text = "é©¬åŒ–è…¾æ˜¯è…¾è®¯å…¬å¸çš„åˆ›å§‹äººï¼Œå…¬å¸æ€»éƒ¨ä½äºæ·±åœ³ã€‚"
    print(f"\næ–‡æœ¬: {chinese_text}")
    result = client.extract_entities(chinese_text)
    print(f"è¯­è¨€: {result['language']}")
    print(f"å®ä½“:")
    for entity in result['entities']:
        print(f"  - {entity['text']} ({entity['type']}) [{entity['confidence']:.2f}]")
    print(f"å…³ç³»:")
    for relation in result['relations']:
        print(f"  - {relation['subject']['text']} --{relation['predicate']}--> {relation['object']['text']}")
    
    # è‹±æ–‡ç¤ºä¾‹
    english_text = "Elon Musk founded SpaceX in California. The company is located in Hawthorne."
    print(f"\nText: {english_text}")
    result = client.extract_entities(english_text)
    print(f"Language: {result['language']}")
    print(f"Entities:")
    for entity in result['entities']:
        print(f"  - {entity['text']} ({entity['type']}) [{entity['confidence']:.2f}]")


def demo_embedding(client: MLServiceClient):
    """Embeddingæ¼”ç¤º"""
    print("\n" + "="*50)
    print("ğŸ“Š Embeddingè¯­ä¹‰åˆ†ææ¼”ç¤º")
    print("="*50)
    
    # ç›¸ä¼¼åº¦è®¡ç®—
    print("\nç›¸ä¼¼åº¦è®¡ç®—:")
    pairs = [
        ("æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "),
        ("æœºå™¨å­¦ä¹ ", "è‹¹æœé¦™è•‰"),
        ("è‡ªç„¶è¯­è¨€å¤„ç†", "NLP"),
    ]
    for t1, t2 in pairs:
        sim = client.similarity(t1, t2)
        print(f"  '{t1}' vs '{t2}': {sim:.4f}")
    
    # è¯­ä¹‰æœç´¢
    print("\nè¯­ä¹‰æœç´¢:")
    docs = [
        {"id": 1, "title": "Pythonæ•™ç¨‹", "text": "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€"},
        {"id": 2, "title": "JavaScript", "text": "JavaScriptç”¨äºç½‘é¡µå¼€å‘"},
        {"id": 3, "title": "æœºå™¨å­¦ä¹ ", "text": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯"},
        {"id": 4, "title": "æ·±åº¦å­¦ä¹ ", "text": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ "},
    ]
    results = client.search("AIæŠ€æœ¯", docs, top_k=2)
    for r in results:
        print(f"  {r['rank']}. {r['title']}: {r['text']} (ç›¸ä¼¼åº¦: {r['similarity']:.4f})")


def demo_llm(client: MLServiceClient):
    """LLMæ¼”ç¤º"""
    print("\n" + "="*50)
    print("ğŸ¤– LLMèŠå¤©æ¼”ç¤º")
    print("="*50)
    
    # ç®€å•å¯¹è¯
    print("\nç®€å•å¯¹è¯:")
    try:
        response = client.chat("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±", provider="ollama")
        print(f"åŠ©æ‰‹: {response}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    # æµå¼è¾“å‡º
    print("\næµå¼è¾“å‡º (ä½¿ç”¨OpenAI):")
    try:
        client.chat("å†™ä¸€é¦–å…³äºAIçš„çŸ­è¯—", provider="openai", stream=True)
    except Exception as e:
        print(f"é”™è¯¯: {e}")


def demo_ontology(client: MLServiceClient):
    """Ontologyæ¼”ç¤º"""
    print("\n" + "="*50)
    print("ğŸ§  OntologyçŸ¥è¯†å›¾è°±æ¼”ç¤º")
    print("="*50)
    
    # æ·»åŠ å®ä½“
    print("\næ·»åŠ å®ä½“åˆ°Ontology:")
    entities = [
        ("OpenAI", "ORGANIZATION", {"founded": 2015}),
        ("GPT-4", "PRODUCT", {"type": "LLM"}),
        ("Sam Altman", "PERSON", {"role": "CEO"}),
    ]
    for text, etype, meta in entities:
        result = client.add_to_ontology(text, etype, meta)
        print(f"  âœ“ {text} ({etype})")
    
    # è·å–ç»Ÿè®¡
    print("\nOntologyç»Ÿè®¡:")
    stats = client.get_ontology_stats()
    print(f"  æ€»å®ä½“æ•°: {stats.get('total_entities', 0)}")
    print(f"  æ€»å…³ç³»æ•°: {stats.get('total_relations', 0)}")
    print(f"  å®ä½“ç±»å‹åˆ†å¸ƒ: {stats.get('entities_by_type', {})}")


def main():
    """ä¸»å‡½æ•°"""
    client = MLServiceClient()
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    print("æ£€æŸ¥æœåŠ¡çŠ¶æ€...")
    try:
        health = client.health()
        print(f"âœ… æœåŠ¡çŠ¶æ€: {health['status']}")
        print(f"æœåŠ¡å¯ç”¨æ€§:")
        for service, available in health['services'].items():
            status = "âœ…" if available else "âŒ"
            print(f"  {status} {service}")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡: {e}")
        print("è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨: python main.py")
        return
    
    # è¿è¡Œæ¼”ç¤º
    demo_ner(client)
    demo_embedding(client)
    demo_ontology(client)
    demo_llm(client)
    
    print("\n" + "="*50)
    print("âœ¨ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("="*50)


if __name__ == "__main__":
    from typing import Union
    main()
